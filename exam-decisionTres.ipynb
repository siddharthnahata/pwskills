{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb1ce1e",
   "metadata": {},
   "source": [
    "Question 1:  What is a Decision Tree, and how does it work in the context of classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3abfb8",
   "metadata": {},
   "source": [
    "Ans- A decision tree is a supervised learning algorithm, used for both classification and regression, that models decisions and their possible consequences as a tree-like structure. In classification, it works by recursively partitioning data into subsets based on feature values, ultimately assigning each subset (or leaf) to a specific class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea2fab",
   "metadata": {},
   "source": [
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074be28",
   "metadata": {},
   "source": [
    "Ans- Gini Impurity and Entropy are both measures of impurity used in decision tree algorithms to determine the best way to split data at each node. They quantify how mixed or pure a node is with respect to the target variable. Lower impurity values indicate a more homogenous node (better split), while higher values indicate a more mixed node (worse split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1d364",
   "metadata": {},
   "source": [
    "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c70152",
   "metadata": {},
   "source": [
    "Ans- Pre-pruning and post-pruning are techniques used to prevent overfitting in decision trees by simplifying the tree structure. Pre-pruning stops the tree from growing during construction based on certain criteria, while post-pruning removes branches from a fully grown tree. A practical advantage of pre-pruning is faster training due to smaller tree size, while post-pruning offers a more robust approach by evaluating the tree's performance after it's fully built.<br>\n",
    "Pre-Tuning Example- A pre-pruning technique could involve stopping the tree from splitting a node if the split doesn't improve the accuracy of the model by a certain threshold or if the resulting leaf node would have fewer than a specified number of samples.<br>\n",
    "Post-Tuning Example- Post-pruning techniques might involve replacing subtrees with leaf nodes if the subtree's error rate is higher than a certain threshold or if the subtree contains too few samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef5ca9",
   "metadata": {},
   "source": [
    "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09181df",
   "metadata": {},
   "source": [
    "Ans- Information gain in decision trees measures how much a particular feature reduces uncertainty (or entropy) when used to split the data. It's crucial for selecting the best split because the feature with the highest information gain will create the most informative split, leading to a more accurate and efficient decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e72a31",
   "metadata": {},
   "source": [
    "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c9adc",
   "metadata": {},
   "source": [
    "Ans- Decision trees are widely used in various real-world applications due to their simplicity and interpretability. They are valuable in areas like loan approval, medical diagnosis, customer churn prediction, and fraud detection. While decision trees offer advantages like ease of understanding and handling both numerical and categorical data, they also have limitations, including potential overfitting and instability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03f9d3",
   "metadata": {},
   "source": [
    "Question 6:   Write a Python program to:\n",
    "<br>●\tLoad the Iris Dataset \n",
    "<br>●\tTrain a Decision Tree Classifier using the Gini criterion \n",
    "<br>●\tPrint the model’s accuracy and feature importances \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a0220eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9556\n",
      "Feature Importace....\n",
      "sepal length (cm): 2.15%\n",
      "sepal width (cm): 2.15%\n",
      "petal length (cm): 57.2%\n",
      "petal width (cm): 38.51%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Target'] = data.target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy Score: {round(accuracy_score(y_test, y_pred), 4)}')\n",
    "\n",
    "print(\"Feature Importace....\")\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    print(f\"{X.columns[i]}: {round(clf.feature_importances_[i] * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb2f72",
   "metadata": {},
   "source": [
    "Question 7:  Write a Python program to:\n",
    "<br>●\tLoad the Iris Dataset \n",
    "<br>●\tTrain a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1d2fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Full grown tree: 0.9556\n",
      "Accuracy Score for 3 branch tree: 0.9556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Target'] = data.target\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "# Model 1: Full tree\n",
    "clf_one = DecisionTreeClassifier()\n",
    "clf_one.fit(X_train, y_train)\n",
    "y_pred_one = clf_one.predict(X_test)\n",
    "clf_one_acc = accuracy_score(y_test, y_pred_one)\n",
    "\n",
    "# Model 2: Tree with max_depth=3\n",
    "clf_two = DecisionTreeClassifier(max_depth=3)\n",
    "clf_two.fit(X_train, y_train)\n",
    "y_pred_two = clf_two.predict(X_test)\n",
    "clf_two_acc = accuracy_score(y_test, y_pred_two)\n",
    "\n",
    "# Results\n",
    "print(f\"Accuracy Score for Full grown tree: {round(clf_one_acc, 4)}\")\n",
    "print(f\"Accuracy Score for 3 branch tree: {round(clf_two_acc, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53730965",
   "metadata": {},
   "source": [
    "Question 8: Write a Python program to: \n",
    "<br>●\tLoad the Boston Housing Dataset \n",
    "<br>●\tTrain a Decision Tree Regressor \n",
    "<br>●\tPrint the Mean Squared Error (MSE) and feature importances \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba75ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5280\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5235\n",
      "HouseAge: 0.0521\n",
      "AveRooms: 0.0494\n",
      "AveBedrms: 0.0250\n",
      "Population: 0.0322\n",
      "AveOccup: 0.1390\n",
      "Latitude: 0.0900\n",
      "Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load Boston Housing Dataset\n",
    "boston = fetch_california_housing()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['MEDV'] = boston.target\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('MEDV', axis=1)\n",
    "y = df['MEDV']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b5d5e",
   "metadata": {},
   "source": [
    "Question 9: Write a Python program to: \n",
    "<br>●\tLoad the Iris Dataset \n",
    "<br>●\tTune the Decision Tree’s max_depth and min_samples_split using \n",
    "GridSearchCV \n",
    "<br>●\tPrint the best parameters and the resulting model accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1543f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 6}\n",
      "Best Model Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['Target'] = iris.target\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76383128",
   "metadata": {},
   "source": [
    "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. \n",
    " Explain the step-by-step process you would follow to: \n",
    "<br>●\tHandle the missing values \n",
    "<br>●\tEncode the categorical features \n",
    "<br>●\tTrain a Decision Tree model \n",
    "<br>●\tTune its hyperparameters \n",
    "<br>●\tEvaluate its performance \n",
    "And describe what business value this model could provide in the real-world setting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed782175",
   "metadata": {},
   "source": [
    "Ans- \n",
    "1. **Handle Missing Values**  \n",
    "   - For numeric features → fill with mean or median.  \n",
    "   - For categorical features → fill with the most frequent category.\n",
    "\n",
    "2. **Encode Categorical Features**  \n",
    "   - Convert categories to numbers using label encoding or one-hot encoding.\n",
    "\n",
    "3. **Train Decision Tree Model**  \n",
    "   - Split the dataset into training and testing sets.  \n",
    "   - Train the Decision Tree using the training set.\n",
    "\n",
    "4. **Tune Hyperparameters**  \n",
    "   - Use GridSearchCV or RandomizedSearchCV to find the best  \n",
    "     `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n",
    "\n",
    "5. **Evaluate Performance**  \n",
    "   - Test the model on the test set using accuracy, precision, recall,  \n",
    "     F1-score, and ROC-AUC for classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Business Value\n",
    "\n",
    "- **Early Detection:** Helps doctors identify high-risk patients sooner.  \n",
    "- **Efficiency:** Reduces time spent on manual diagnosis.  \n",
    "- **Cost Savings:** Early treatment is cheaper and more effective.  \n",
    "- **Improved Outcomes:** Leads to better patient recovery rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
