{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2afdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933b520",
   "metadata": {},
   "source": [
    "1.  What is Ensemble Learning in machine learning? Explain the key idea \n",
    "behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b58cd3",
   "metadata": {},
   "source": [
    "Ans- Ensemble learning is a ML program where we train multiple models in a single and select the best model by comparing or taking an average. It makes the model more accurate and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4ffac",
   "metadata": {},
   "source": [
    "2. What is the difference between Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b91ca",
   "metadata": {},
   "source": [
    "Ans- In the bagging method all the individual models will take the bootstrap samples and create the models in parallel. Whereas in the boosting each model will build sequentially. The output of the first model (the errors information) will be pass along with the bootstrap samples data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd9e6e",
   "metadata": {},
   "source": [
    "3.  What is bootstrap sampling and what role does it play in Bagging methods \n",
    "like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf1e2b",
   "metadata": {},
   "source": [
    "Ans- Bootstrapping is a key component of Bagging (Bootstrap Aggregating), where multiple models are trained on different samples and combined for better accuracy. Random Forests, a popular machine learning algorithm, use bootstrapping to train multiple decision trees and aggregate their results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a3bea",
   "metadata": {},
   "source": [
    "4.  What are Out-of-Bag (OOB) samples and how is OOB score used to \n",
    "evaluate ensemble models? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937edd20",
   "metadata": {},
   "source": [
    "Ans- OOB is part of train data but not used in an individual decision tree and the data is used as validation data for the individual decision tree. At the end average of all the score it is called OOB score for the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0885d1",
   "metadata": {},
   "source": [
    "5. Compare feature importance analysis in a single Decision Tree vs. a \n",
    "Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2981e",
   "metadata": {},
   "source": [
    "Ans- In a single Tree the output is single model is used on whole and whereas random forest the data is being divided in further subsets which make impurity averaged at the end which make feature more important and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c16cc",
   "metadata": {},
   "source": [
    "6.  Write a Python program to: \n",
    "● Load the Breast Cancer dataset using \n",
    "sklearn.datasets.load_breast_cancer() \n",
    "● Train a Random Forest Classifier \n",
    "● Print the top 5 most important features based on feature importance scores. \n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e15f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important features:\n",
      "                Feature  Importance\n",
      "0            worst area    0.139357\n",
      "1  worst concave points    0.132225\n",
      "2   mean concave points    0.107046\n",
      "3          worst radius    0.082848\n",
      "4       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "})\n",
    "\n",
    "top5 = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
    "top5.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Top 5 most important features:\")\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56739fb9",
   "metadata": {},
   "source": [
    "7.  Write a Python program to: \n",
    "● Train a Bagging Classifier using Decision Trees on the Iris dataset \n",
    "● Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9633283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Single Decision Tree: 1.0\n",
      "Accuracy of Bagging Classifier: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# Results\n",
    "print(\"Accuracy of Single Decision Tree:\", acc_dt)\n",
    "print(\"Accuracy of Bagging Classifier:\", acc_bag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db5163",
   "metadata": {},
   "source": [
    "8. Write a Python program to: \n",
    "● Train a Random Forest Classifier \n",
    "● Tune hyperparameters max_depth and n_estimators using GridSearchCV \n",
    "● Print the best parameters and final accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545dd864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best Parameters:  {'max_depth': None, 'n_estimators': 200}\n",
      "Best Score:  0.959746835443038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(100, 201, 25)),\n",
    "    'max_depth': list(range(1, 10, 2)) + [None]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    verbose=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", rf_grid.best_params_)\n",
    "print(\"Best Score: \", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e938d4",
   "metadata": {},
   "source": [
    "9. Write a Python program to: \n",
    "● Train a Bagging Regressor and a Random Forest Regressor on the California \n",
    "Housing dataset \n",
    "● Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "636a6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 10 for this parallel run (total 10)...\n",
      "Building estimator 2 of 10 for this parallel run (total 10)...\n",
      "Building estimator 3 of 10 for this parallel run (total 10)...\n",
      "Building estimator 4 of 10 for this parallel run (total 10)...\n",
      "Building estimator 5 of 10 for this parallel run (total 10)...\n",
      "Building estimator 6 of 10 for this parallel run (total 10)...\n",
      "Building estimator 7 of 10 for this parallel run (total 10)...\n",
      "Building estimator 8 of 10 for this parallel run (total 10)...\n",
      "Building estimator 9 of 10 for this parallel run (total 10)...\n",
      "Building estimator 10 of 10 for this parallel run (total 10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "Bagging Model MSE: 0.2824242776841025\n",
      "Random Forest Model MSE: 0.28422271730492676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "dt = DecisionTreeRegressor()\n",
    "bag = BaggingRegressor(\n",
    "    estimator=dt,\n",
    "    random_state=42,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred_bag = bag.predict(X_test)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(f\"Bagging Model MSE: {mean_squared_error(y_test, y_pred_bag)}\")\n",
    "print(f\"Random Forest Model MSE: {mean_squared_error(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3076f",
   "metadata": {},
   "source": [
    "10.  You are working as a data scientist at a financial institution to predict loan \n",
    "default. You have access to customer demographic and transaction history data. \n",
    "You decide to use ensemble techniques to increase model performance. \n",
    "Explain your step-by-step approach to: \n",
    "● Choose between Bagging or Boosting \n",
    "● Handle overfitting \n",
    "● Select base models \n",
    "● Evaluate performance using cross-validation \n",
    "● Justify how ensemble learning improves decision-making in this real-world \n",
    "context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00021a",
   "metadata": {},
   "source": [
    "### Loan Default Prediction using Ensemble Learning\n",
    "\n",
    "**Step 1: Choose between Bagging or Boosting**\n",
    "- **Bagging (Random Forest):** Reduces variance, good baseline, stable performance.\n",
    "- **Boosting (XGBoost/LightGBM):** Reduces bias, sequentially improves weak learners, usually higher accuracy.\n",
    "\n",
    "**Step 2: Handle Overfitting**\n",
    "- Use cross-validation (Stratified K-Fold).\n",
    "- Apply regularization (max_depth, min_samples_split, learning rate, early stopping).\n",
    "- Handle class imbalance (SMOTE, class weights).\n",
    "\n",
    "**Step 3: Select Base Models**\n",
    "- Decision Trees → common base learner.\n",
    "- Random Forest for baseline.\n",
    "- XGBoost/LightGBM for final tuned model.\n",
    "\n",
    "**Step 4: Evaluate Performance**\n",
    "- Metrics: AUC-ROC, Precision, Recall, F1-score.\n",
    "- Use Stratified K-Fold CV for balanced evaluation.\n",
    "\n",
    "**Step 5: Why Ensemble Learning Helps**\n",
    "- Combines multiple models → reduces variance & bias.\n",
    "- Captures complex non-linear patterns in customer data.\n",
    "- Provides robust, accurate, and interpretable results.\n",
    "- Improves decision-making, reduces false approvals, and lowers financial risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b764f09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
